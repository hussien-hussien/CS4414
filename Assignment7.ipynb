# %% markdown
# # Assignment 6: Classification with Logistic Regression
#
# # Total: 20 pts
#
# ## Instructions
#
# * Complete the assignment
#
# * Once the notebook is complete, restart your kernel and rerun your cells
#
# * Submit this notebook to owl by the deadline
#
# ## The Dataset
#
# This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.
#
# You can read more about the data and the variables [here](https://www.kaggle.com/uciml/pima-indians-diabetes-database).
# %% codecell
# You may need these
import pandas as pd
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import recall_score, make_scorer, confusion_matrix
from sklearn.utils.multiclass import unique_labels
from sklearn.preprocessing import StandardScaler

import matplotlib.pyplot as plt
%matplotlib inline
# %% markdown
# ## Question 1: 1 pt
#
# Read in the `diabetes.csv` dataset. How many variables and how many observations?
#
# %% codecell
df = pd.read_csv('diabetes.csv')
df.shape
# %% markdown
# ## Question 2: 1 pt
#
# Then split the data into train and test for the outcome and the predictor variables.  Hold out 50% of observations as the test set.  Pass `random_state=0` to `train_test_split` to ensure you get the same train and tests sets as the solution.
# %% codecell

# %% markdown
# ## Question 3: 1 pts
#
# Read the documentation for [sklearn's `LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).  In no more than 2 sentences per bullet point, answer the following in your own words.
#
# * Does `LogisticRegression` use a penalty by default?  If yes, what penalty?  If it does, does this mean that `LogisticRegression` actually uses ridge regression or the LASSO as the default?
#
# * What is the interpretation of the paramater `C`?  How does it relate to the regularization strength $\lambda$?
#
#
# * If I were to ask you to use a regularization strengh of 2 (i.e. $\lambda=2$), what value of `C` would you pass?
#
# Answer in the cell below using markdown
# %% markdown
# Answers go here!
# * Yes
# *
# %% markdown
# ## Question 4: 3 pts
#
# Create a instance of sklearn's `LogisticRegression` object for simple logistic regression (that is, the unpenalized version).  You will need to choose an alternative solver for `LogisticRegression` since the default solver does not support the no penalty option. Any solver will do, so just take a look at the docs to see what is available. I used `solver="newton-cg"` which seems to work fine.
#
# Note: If you get a warning about convergence of `coef_`, try increasing the `max_iter` parameter.  I used `max_iter=10000` which seems to supress the warning.
#
# Using this object, run a logisitic regression analysis of `Outcome` (y-variable) against `Glucose` (x-variable) on your training data. Make a scatter-plot of x and y and add the class prediction (0 or 1, using `predict`) and the predicted probability of a positive outcome (using `predict_proba`). Note that `predict_proba` will return both p(Outcome=0) and p(Outcome=1).
# %% markdown
# ## Question 5: 1 pt
# Using the normal logistic regression from Question 4 to fit a model to predict outcome from all the variables in the data frame. Report the coefficients. Which variable increases the probability of having diabetes, and which variables decrease the probability of having diabetes.
# %% markdown
# ## Question 6: 3 pts
#
# Use your model to construct a confusion matrix by fitting and predicting on the training data (I've inlcluded a little helper function to make looking at the confusion matrix a little easier). Then answer the following using the confusion matrix (don't use sklearn's functions):
#
# * What is your model's training accuracy?
# * What is your model's training precision?
# * What is your model's training recall?
# %% codecell
def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
    classes = classes[unique_labels(y_true, y_pred)]
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    fig, ax = plt.subplots()
    ax.axis('equal')
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout();
    return ax
# %% markdown
# ## Question 7:  1 pts
#
# Estimate logistic regression's out of sample recall by using 5 fold cross validation.
# %% markdown
# ## Question 8: 1 pt
#
# Create new pipelines for l2 penalty in logistic regression and an l1 penalty.  Remember, penalized models perform best when you scale the inputs.  You should add `StandardScaler()` to your pipeline.
# %% markdown
# ## Question 9: 3 pts
#
#   Use sklearn's `GridSearchCV` to search over the regularization strength ranging from 0.1 to 1000 in 30 evenly spaced increments for your models. Vary the parameter evenly in log-space. Use recall as your metric for scoring.
# Plot the score for both lasso and ridge as a function of the log-regularization parameter.
#
# `GridSearchCV` is a way to cross validate your models for a variety of parameters.  Read more about `GridSearchCV` [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).
# %% markdown
# ## Question 10: 1 pt
#
# Print the cross validated recall for your regularized models.  If you called your model grid search `lasso_gscv` you can access the best model's score by performing `lasso_gscv.best_score_`.
#
# %% markdown
# ## Question 11: 4 pts
#
# LASSO makes the assumption that the model is what we call *sparse* (that means, not every variable is actually related to the outcome).  We can see which variables are more important than others by examining what is known as the *coefficient paths*.
#
# Here are the steps to create the coefficient path:
#
# 1) Initialize an array of regularization strengths (typically going from something very small, maybe 0.1, to something very large, maybe 1000.
#
# 2) For each regularization strength, fit your model.  Keep track of the coefficients somehow, either by initializing an empty array to store the coefficients or by appending them to a list.
#
# 3) Plot the coefficient values against the log of the regularization strength.
#
# For an example + example code - see Lab05 - last item.
#
# You've done this correctly if you can create a plot which looks like [this](https://cvxpy.readthedocs.io/en/latest/_images/lasso_regression_11_0.svg).
#
# * Construct the coefficient path for logistic regression with an l1 penalty.
# * Determine which coefficient is most strongly related to the outcome by examining which coefficent reaches 0 last.
#
# %% codecell

# %% codecell
