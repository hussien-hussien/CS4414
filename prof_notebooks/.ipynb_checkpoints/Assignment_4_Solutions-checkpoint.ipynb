{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grade: /20 Mark(s)\n",
    "\n",
    "# Assignment 04: Model Selection & Cross Validation\n",
    "\n",
    "### You're a Data Scientist!\n",
    "You are working as a Junior Data Scientist for a professional football (er, Soccer) club.  The owner of the team is very interested in seeing how the use of data can help improve the team's peformance, and perhaps win them a championship!\n",
    "\n",
    "The draft is coming up soon (thats when you get to pick new players for your team), and the owner has asked you to create a model to help score potential draftees.  The model should look at attributes about the player and predict what their \"rating\" will be once they start playing professionally.\n",
    "\n",
    "The football club's data team has provided you with data for 17,993 footballers from the league.  Your job: work with the Senior Data Scientist to build a model or models, perform model selection, and make predictions on players you have not yet seen.\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "The data is stored in a csv file called `footballer_data.csv`.  The data contain 52 columns, including some information about the player, their skills, and their overall measure as an effective footballer.\n",
    "\n",
    "Most features relate to the player's abilities in football related skills, such as passing, shooting, dribbling, etc.  Some features are rated on a 1-5 scale (5 being the best), others are rated on 0-100 (100 being the best), and others still are categorical (e.g. work rate is coded as low, medium, or high).\n",
    "\n",
    "The target variable (or $y$ variable, if you will) is `overall`.  This is an overall measure of the footballer's skill and is rated from 0 to 100.  The most amazingly skilled footballer would be rated 100, where as I would struggle to score more than a 20. The model(s) you build should use the other features to predict `overall`.\n",
    "\n",
    "\n",
    "### Follow These Steps before submitting\n",
    "Once you are finished, ensure to complete the following steps.\n",
    "\n",
    "1.  Restart your kernel by clicking 'Kernel' > 'Restart & Run All'.\n",
    "\n",
    "2.  Fix any errors which result from this.\n",
    "\n",
    "3.  Repeat steps 1. and 2. until your notebook runs without errors.\n",
    "\n",
    "4.  Submit your completed notebook to OWL by the deadline.\n",
    "\n",
    "\n",
    "### Preliminaries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: /2 Mark(s)\n",
    "\n",
    "Read in the data and take a look at the dataframe.  There should be 52 columns. The outcome of interest is called `overall` which gives an overall measure of player performance. Not all of the other columns are particularly useful for modelling though (for instance, `ID` is just a unique identifier for the player.  This is essentially an arbitrary number and has no bearing on the player's rating).\n",
    "\n",
    "The Senior Data Scientist thinks the following columns should be removed:\n",
    "\n",
    "* ID\n",
    "* club\n",
    "* club_logo\n",
    "* birth_date\n",
    "* flag\n",
    "* nationality\n",
    "* photo\n",
    "* potential\n",
    "\n",
    "The Senior Data Scientist would also like the following columns converted into dummy variables:\n",
    "\n",
    "* work_rate_att\n",
    "* work_rate_def\n",
    "* preferred_foot\n",
    "\n",
    "Clean the data according to the Senior Data Scientist's instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'footballer_data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e4a597e712f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'footballer_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Drop the aformentioned columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'club'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'club_logo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'flag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nationality'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'photo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'potential'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'birth_date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'columns'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'footballer_data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('footballer_data.csv')\n",
    "\n",
    "# Drop the aformentioned columns\n",
    "model_data = df.drop(['ID','club','club_logo','flag', 'nationality','photo','potential', 'birth_date'], axis = 'columns')\n",
    "\n",
    "# In order to get dummies, convert categorical data to categorical type\n",
    "model_data['work_rate_att'] = pd.Categorical(model_data.work_rate_att, categories=['Low','Medium','High'])\n",
    "model_data['work_rate_def'] = pd.Categorical(model_data.work_rate_def, categories=['Low','Medium','High'])\n",
    "model_data['preferred_foot'] = pd.Categorical(model_data.preferred_foot, categories = ['Left','Right'])\n",
    "\n",
    "# Dummies, dropping the first category\n",
    "model_data = pd.get_dummies(model_data, drop_first=True)\n",
    "\n",
    "model_data.head()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: /1 Mark(s)\n",
    "\n",
    "The data should all be numerical now. Before we begin modelling, it is important to obtain a baseline for the accuracy of our predictive models. Compute the absolute errors resulting if we use the median of the `overall` variable to make predictions. This will serve as our baseline performance. Plot the distribution of the errors and print their mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "loss = abs(model_data.overall - model_data.overall.median())\n",
    "sns.distplot(loss)\n",
    "\n",
    "mu = loss.mean()\n",
    "sigma = loss.std()\n",
    "\n",
    "print(mu.round(2), sigma.round(2))\n",
    "\n",
    "x = np.random.normal(10,2,size = 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: /3 Mark(s)\n",
    "To prepare the data for modelling, the Senior Data Scientist recomends you use `sklearn.model_selection.train_test_split` to seperate the data into a training set and a test set.\n",
    "\n",
    "The Senior Data Scientist would like to estimate the performance of the final selected model to within +/- 0.25 units using mean absolute error as the loss function of choice.  Decide on an appropriate size for the test set, then use `train_test_split` to split the features and target variables into appropriate sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need rationale or sample size\n",
    "# Need an estimate for sigma \n",
    "# Use sigma from previous question\n",
    "d = 0.25 #Provided from question\n",
    "test_size = (2*sigma/d)**2\n",
    "\n",
    "y = model_data.overall\n",
    "X = model_data.drop('overall', axis = 'columns')\n",
    "\n",
    "# Random state assures that folds are consistent across models\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,\n",
    "                                                y, \n",
    "                                                test_size = int(np.ceil(test_size)), \n",
    "                                                random_state = 0)\n",
    "\n",
    "print(Xtrain.shape,Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: /1 Mark(s)\n",
    "\n",
    "\n",
    "The Senior Data Scientist wants you to fit a linear regression to the data as a first model.  Use sklearn to build a model pipeline which fits a linear regression to the data. (This will be a very simple, one-step pipeline but we will expand it later.) You can read up on sklearn pipelines [here](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). Note that the sklearn linear regression adds its own intercept so you don't need to create a column of 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline([\n",
    "    ('linear_regression', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: /3 Mark(s)\n",
    "\n",
    "The senior data scientist wants a report of this model's cross validation score.  Use 5 fold cross validation to estimate the out of sample performance for this model.  You may find sklearn's `cross_val_score` useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = cross_val_score(model_pipeline, Xtrain, ytrain, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# cross_val_score uses \"scorers\" for which higher is better\n",
    "cv_score_mean = -cv_score.mean()\n",
    "\n",
    "print(cv_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: /3 Mark(s)\n",
    "\n",
    "That's impressive!  Your model seems to be very accurate, but now the Senior Data Scientist wants to try and make it more accurate.  Scouts have shared with the Senior Data Scientist that players hit their prime in their late 20s, and as they age they become worse overall.\n",
    "\n",
    "The Senior Data Scientist wants to add a quadratic term for age to the model.  Repeat the steps above (creating a pipeline, validating the model, etc) for a model which includes a quadratic term for age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is one way to do this question\n",
    "# Essentially, we manually add the age squared feature to our dataset\n",
    "# and proceed as usual\n",
    "y_quad = model_data.overall\n",
    "X_quad = model_data.assign(age2 = lambda x: x.age**2).drop('overall', axis = 'columns')\n",
    "\n",
    "X_quad_train, X_quad_test, y_quad_train, y_quad_test = train_test_split(X_quad, y_quad, \n",
    "                                                                        test_size = test_size, \n",
    "                                                                        random_state = 0)\n",
    "\n",
    "quadratic_model_pipeline = Pipeline([\n",
    "    ('linear_regression', LinearRegression())\n",
    "])\n",
    "\n",
    "quad_cv_score = cross_val_score(quadratic_model_pipeline, \n",
    "                                X_quad_train, \n",
    "                                y_quad_train, \n",
    "                                cv = 5, \n",
    "                                scoring = 'neg_mean_absolute_error')\n",
    "\n",
    "# cross_val_score uses \"scorers\" for which higher is better\n",
    "quad_cv_score_mean = -quad_cv_score.mean()\n",
    "\n",
    "print(quad_cv_score_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is another way using ColumnTransformer\n",
    "# ColumnTransformer is a way to apply transforms to single columns or groups of columns\n",
    "# without having to manually engineer the new feature.  It works a lot like Pipeline.\n",
    "# This is nice because we can pass our raw data and not have to worry about computing the correct feature\n",
    "# This sounds silly for something as easy as squaring a column, but when your features become more complex\n",
    "# this becomes very usefil\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Using PolynomialFeatures to square the age column.\n",
    "# The tuple looks like ('name_of_step', Transformer, ['column_you_want_to_transform'])\n",
    "list_of_transforms = [('square_age', PolynomialFeatures(include_bias=False, interaction_only=False), ['age'])]\n",
    "\n",
    "#remainder='passthrough' means that untransformed features are left alone and are not altered or thrown away.  Very important!!!\n",
    "ct = ColumnTransformer(list_of_transforms, remainder='passthrough')\n",
    "\n",
    "#Inclide as a part of our pipeline\n",
    "model_with_age_squared = Pipeline([\n",
    "    ('column_transformer', ct),\n",
    "    ('linear_regression', LinearRegression())\n",
    "])\n",
    "\n",
    "#Proceed as usual.  Note I am passing the training data without the squared age column!\n",
    "cv_score = cross_val_score(model_with_age_squared,\n",
    "                          Xtrain,\n",
    "                          ytrain,\n",
    "                          cv=5,\n",
    "                          scoring='neg_mean_absolute_error')\n",
    "\n",
    "cv_score_mean = -cv_score.mean()\n",
    "\n",
    "# Same Score as above.  A lille extra work for a lot of extra freedom and no worry!\n",
    "print(cv_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: /3 Mark(s)\n",
    "\n",
    "\n",
    "The Senior Data Scientist isn't too happy that the quadratic term has not improved the fit of the model much and now wants to include quadratic and interaction term for every feature (That's a total of 1080 features!!!!)\n",
    "\n",
    "Add sklearn's `PolynomialFeatures` to your pipeline from part C.  Report the cross validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline_intxn = Pipeline([\n",
    "    ('poly', PolynomialFeatures(include_bias = False)),\n",
    "    ('linear_regression', LinearRegression())\n",
    "])\n",
    "\n",
    "cv_score = cross_val_score(model_pipeline_intxn, \n",
    "                           Xtrain, \n",
    "                           ytrain, \n",
    "                           cv = 5, \n",
    "                           scoring = 'neg_mean_absolute_error')\n",
    "\n",
    "print(-cv_score.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: /2 Mark(s)\n",
    "\n",
    "The Senior Data Scientist is really happy with the results of adding every interaction into the model and wants to explore third order interactions (that is adding cubic terms to the model).\n",
    "\n",
    "This is not a good idea!  Talk them down from the ledge.  Write them an email in the cell below explaining what could happen if you add too may interactions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey Boss,\n",
    "\n",
    "I got your email about adding cubic terms to the model.  I think...adding cubic terms to the model will likely overfit the data.  If we add cubic terms, then the number of predictors will be very close to the number of observations.  This will make it easy for us to get a low training error, but will likely not generalize very well. It is my recomendation that we NOT add cubic terms.\n",
    "\n",
    "Sincerly,\n",
    "\n",
    "Junior Data Scientist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9:  /2 Mark(s)\n",
    "\n",
    "You've successfully talked the Senior Data Scientist out of adding cubic terms to the model. Good job!\n",
    "\n",
    "Based on the cross validation scores, which model would you choose?  Estimate the performance of your chosen model on the test data you held out, and do the following:\n",
    "\n",
    "- Compute a point estimate for the generalization error.\n",
    "- Compute a confidence interval for the generalization error.  \n",
    "- Plot the distribution of the absolute errors.\n",
    "\n",
    "Is the test error close to the cross validation error of the model you chose? Why do you think this is the case?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our best performing model on all the training data\n",
    "model_pipeline_intxn.fit(Xtrain, ytrain)\n",
    "\n",
    "#Predict on the test set\n",
    "ypred = model_pipeline_intxn.predict(Xtest)\n",
    "\n",
    "#Compute the errors and a point estimate of the generalization error\n",
    "test_errors = np.abs(ytest - ypred)\n",
    "generalization_error = test_errors.mean()\n",
    "\n",
    "#Construct a confidence interval\n",
    "# We have enough data in our test set that the appropirate t-quantile is close to 1.96\n",
    "test_ci = generalization_error + 1.96 * np.std(test_errors) / np.sqrt(len(test_errors)) * np.array([-1, 1])\n",
    "\n",
    "sns.distplot(abs(test_errors))\n",
    "\n",
    "print('Generalization Error: ',generalization_error.round(2))\n",
    "print('Confidence interval: ', test_ci.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow These Steps before submitting\n",
    "Once you are finished, ensure to complete the following steps.\n",
    "\n",
    "1.  Restart your kernel by clicking 'Kernel' > 'Restart & Run All'.\n",
    "\n",
    "2.  Fix any errors which result from this.\n",
    "\n",
    "3.  Repeat steps 1. and 2. until your notebook runs without errors.\n",
    "\n",
    "4.  Submit your completed notebook to OWL by the deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
